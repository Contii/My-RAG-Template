# System Architecture

Technical overview of the RAG system components and data flow.

---
## ğŸ—ï¸ System Overview

```mermaid
graph TB
    User[User Input] --> Main[main.py]
    Main --> Pipeline[RAG Pipeline]
    
    Pipeline --> Retriever[Smart Retriever]
    Pipeline --> Generator[Generator Factory]
    Pipeline --> Metrics[Metrics Collector]
    
    Retriever --> Semantic[Semantic Search]
    Retriever --> Rerank[Cross-Encoder]
    Retriever --> Cache[Query Cache]
    
    Generator --> HF[HuggingFace Models]
    
    Metrics --> Track[System/Retrieval/Cache/Generator]
```

**Core Flow:**
```
User Question â†’ Retriever (fetch docs) â†’ Generator (create answer) â†’ Display + Metrics
```

---
## ğŸ“¦ Core Components

### 1. RAG Pipeline

**File:** `pipeline/rag_pipeline.py`

The main orchestrator that connects all components.

```python
class RAGPipeline:
    def __init__(self, config_path="config/config.yaml"):
        # Load configuration
        self.config = yaml.safe_load(config_path)
        
        # Initialize components
        self.retriever = SmartRetriever(...)
        self.generator = GeneratorFactory.create_generator(...)
        self.metrics_collector = MetricsCollector()
    
    def run(self, question, filters=None):
        # 1. Retrieve relevant documents
        context = self.retriever.retrieve(question, filters)
        
        # 2. Generate answer using LLM
        answer, time = self.generator.generate(context, question)
        
        # 3. Track performance
        self.metrics_collector.track_generation(time)
        
        return context, answer
```

**Key Responsibilities:**
- Load and manage configuration from `config/config.yaml`
- Coordinate retrieval â†’ generation workflow
- Handle model switching at runtime
- Aggregate metrics from all components

---
### 2. Smart Retriever

**File:** `retriever/smart_retriever.py`

Unified retrieval component that replaced 4 legacy classes.

**Architecture:**
```python
SmartRetriever(
    use_reranking=True,    # Enable cross-encoder reranking
    use_cache=True,        # Enable query caching
    use_filters=True,      # Enable metadata filtering
    use_metrics=True       # Enable performance tracking
)
```

**Retrieval Process:**

```
1. Check Cache
   â†“ (if miss)
2. Semantic Search (FAISS + Sentence-BERT)
   - Encode query to vector
   - Find top-k similar documents
   â†“
3. Apply Filters (if enabled)
   - Filter by file type (.pdf, .docx, etc)
   - Filter by source file name
   - Filter by date range
   - Filter by minimum similarity score
   â†“
4. Rerank Results (if enabled)
   - Use cross-encoder model
   - Re-score top candidates
   - Select best matches
   â†“
5. Cache Result
   â†“
6. Return to Pipeline
```

**Key Features:**
- **FAISS Index:** Efficient vector similarity search (cosine distance)
- **Sentence-BERT:** `all-MiniLM-L6-v2` for embeddings (384 dimensions)
- **Cross-Encoder:** `ms-marco-MiniLM-L-6-v2` for reranking
- **File Cache:** Stores queries with configurable TTL (default: 1 hour)
- **Metadata Filters:** Flexible filtering on document properties

**Configuration Example:**
```yaml
retriever:
  model_name: "all-MiniLM-L6-v2"
  embeddings_path: "data/embeddings.json"
  top_k: 5                    # Initial candidates
  use_reranking: true
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  rerank_top_k: 3            # Final results after reranking
  cache_enabled: true
  cache_ttl: 3600            # 1 hour in seconds
  metrics_enabled: true
```

---
### 3. Generator Factory

**File:** `generator/generator_factory.py`

Manages multiple LLM models with runtime switching capability.

**Supported Models:**

| Model | Parameters | Quantization | Speed (CPU) | Memory | Use Case |
|-------|-----------|--------------|-------------|--------|----------|
| **Bitnet** | 2B | 4-bit | ~80s | 2GB | General purpose, fast inference |
| **Gemma** | 2-7B | Optional | ~120s | 4-8GB | Instruction following, quality |
| **Llama** | 7-13B | 4/8-bit | ~180s | 8-16GB | Complex reasoning, research |

**Model Configuration:**
```yaml
default_model: "bitnet"

models:
  bitnet:
    path: "microsoft/bitnet-b1.58-2B-4T"
    type: "huggingface"
    quantization: "4bit"               # Reduces memory usage
    max_new_tokens: 250                # Output length limit
    temperature: 0.7                   # Creativity (0.0-1.0)
    
  gemma:
    path: "google/gemma-2b-it"
    type: "huggingface"
    quantization: null                 # No quantization
    max_new_tokens: 512
    temperature: 0.7
```

**Generation Flow:**
```python
1. Build Prompt
   "Context: {retrieved_docs}\nQuestion: {user_question}\nAnswer:"
   â†“
2. Tokenize Input
   Convert text to token IDs
   â†“
3. Model Inference
   Generate tokens autoregressively
   Track: tokens/second, total time
   â†“
4. Decode Output
   Convert token IDs back to text
   â†“
5. Return Answer + Metrics
```

**Runtime Model Switching:**
```python
# Switch without restart
pipeline.switch_model('gemma')

# Automatically:
# - Unloads current model
# - Loads new model from config
# - Preserves retriever and metrics
```

---
### 4. Metrics Collector

**File:** `utils/metrics/unified_metrics.py`

Centralized metrics tracking with unified dashboard.

**Architecture:**
```python
MetricsCollector:
    â”œâ”€â”€ system_monitor: SystemMetrics
    â”‚   â””â”€â”€ Tracks: CPU %, RAM usage, GPU utilization
    â”‚
    â”œâ”€â”€ retrieval_tracker: RetrievalMetrics  
    â”‚   â””â”€â”€ Tracks: Query count, latency, results per query
    â”‚
    â”œâ”€â”€ cache_tracker: CacheMetrics
    â”‚   â””â”€â”€ Tracks: Hit/miss rate, time saved
    â”‚
    â””â”€â”€ generator_tracker: GeneratorMetrics
        â””â”€â”€ Tracks: Generation time, tokens/sec, success rate
```

**Dashboard Output:**
```
============================================================
              UNIFIED RAG METRICS DASHBOARD
============================================================
Session Start: 2025-10-15 14:30:00
============================================================

ğŸ–¥ï¸  SYSTEM RESOURCES:
   CPU: Avg 45.2%, Max 78.5%
   RAM: Avg 4.23GB, Max 6.15GB
   GPU: NVIDIA RTX 3080
        Avg 65.4%, Max 92.3%

ğŸ” RETRIEVAL:
   Total Queries: 15
   Avg Response Time: 0.234s
   Avg Results/Query: 5.0

ğŸ’¾ CACHE:
   Total Requests: 15
   Hit Rate: 40.0%
   Time Saved: 1.42s

ğŸ¤– GENERATOR:
   Total Generations: 15
   Avg Duration: 12.45s
   Avg Speed: 8.2 tokens/s
   Success Rate: 100.0%

============================================================
```

**Access:** Type `metrics` in CLI or call `pipeline.retriever.print_metrics_dashboard()`

---
## ğŸ—‚ï¸ Project Structure

```
My-RAG-Template/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # Central configuration (models, retriever, logging)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ documents/               # Place your documents here (auto-scanned)
â”‚   â””â”€â”€ embeddings.json          # Generated vectors (auto-created)
â”‚
â”œâ”€â”€ generator/
â”‚   â”œâ”€â”€ base_generator.py        # Abstract generator interface
â”‚   â”œâ”€â”€ generator.py             # HuggingFace implementation
â”‚   â””â”€â”€ generator_factory.py     # Multi-model factory pattern
â”‚
â”œâ”€â”€ retriever/
â”‚   â”œâ”€â”€ smart_retriever.py       # Main retriever (unified)
â”‚   â”œâ”€â”€ semantic_retriever.py    # Base vector search (FAISS)
â”‚   â”œâ”€â”€ faiss_index.py           # Vector index management
â”‚   â””â”€â”€ [legacy files]           # Backward compatibility
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ unified_metrics.py   # MetricsCollector
â”‚   â”‚   â”œâ”€â”€ system_metrics.py    # CPU/RAM/GPU tracking
â”‚   â”‚   â”œâ”€â”€ retrieval_metrics.py # Query performance
â”‚   â”‚   â”œâ”€â”€ cache_metrics.py     # Hit rate tracking
â”‚   â”‚   â””â”€â”€ generator_metrics.py # LLM performance
â”‚   â”‚
â”‚   â”œâ”€â”€ document_parsers.py      # Multi-format parsing (8 formats)
â”‚   â””â”€â”€ query_cache.py           # File-based cache (TTL)
â”‚
â”œâ”€â”€ logger/
â”‚   â””â”€â”€ logger.py                # Centralized logging system
â”‚
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ rag_pipeline.py          # Main orchestrator
â”‚
â””â”€â”€ main.py                      # CLI entry point
```

---
## ğŸ“Š Performance Characteristics

### Typical Query Latency (5 documents, 250 tokens)

| Component | CPU (no cache) | CPU (cached) | GPU |
|-----------|---------------|--------------|-----|
| Retrieval | 200ms | 5ms | 150ms |
| Reranking | 100ms | - | 50ms |
| Generation | 80s | 80s | 8s |
| **Total** | **~80.3s** | **~80s** | **~8.2s** |

### Memory Usage by Configuration

| Setup | RAM | Notes |
|-------|-----|-------|
| Bitnet 4-bit | ~2GB | Recommended for CPU |
| Gemma no quant | ~4GB | Better quality |
| Llama 8-bit | ~8GB | Best reasoning |
| Embeddings (1000 docs) | ~50MB | Persistent |

---
## ğŸ¯ Design Decisions

### Why SmartRetriever?

**Problem:** Original architecture had 4 separate retriever classes chained together:
```python
semantic â†’ reranking â†’ cache â†’ filtered
```

**Solution:** Single `SmartRetriever` with toggleable features.

**Benefits:**
- âœ… Simpler initialization (1 class vs 4)
- âœ… Easier testing (mock once vs 4 times)
- âœ… Better performance (shared state)
- âœ… Flexible configuration (enable/disable features)

### Why Generator Factory?

**Benefits:**
- âœ… Runtime model switching without restart
- âœ… Easy to add new models (just edit config)
- âœ… Consistent interface across models
- âœ… Configuration-driven (no code changes)

### Why Unified Metrics?

**Problem:** Metrics were scattered across components, hard to aggregate.

**Solution:** `MetricsCollector` as single source of truth.

**Benefits:**
- âœ… Consistent tracking API
- âœ… Unified dashboard view
- âœ… Performance insights (automatic recommendations)
- âœ… Easy to extend (add new metric tracker)

---
## ğŸ“š Documentation

- [Installation Guide](docs/INSTALL.md) - Detailed setup instructions
- [Usage Examples](docs/USAGE_EXAMPLES.md) - Practical examples and use cases
- [HuggingFace Models Configuration](docs/HUGGINGFACE_CONFIGS.md) - Detailed HuggingFace configuration parameters