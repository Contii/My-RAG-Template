from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


class GeneratorStub:
    """
    Stub generator that returns a static answer.
    """

    def generate(self, context, question):
        return "Answer generated by the LLM (stub)"


class LLMGenerator:
    """
    Generator that uses a Hugging Face LLM to generate answers.
    """

    def __init__(self, model_id, max_tokens=250, temperature=0.7):
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id, torch_dtype=torch.bfloat16, device_map="auto"
            )
        except Exception as e:
            raise RuntimeError(f"Failed to load LLM model '{model_id}': {e}")
        self.max_tokens = max_tokens
        self.temperature = temperature

    def generate(self, context, question):
        try:
            if context and any(context):
                prompt = f"<|begin_of_text|>Context: {' '.join(context)}<|eot_id|>User: {question}<|eot_id|>Assistant: "
            else:
                prompt = f"<|begin_of_text|>User: {question}<|eot_id|>Assistant: "
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            output = self.model.generate(
                **inputs,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                pad_token_id=self.tokenizer.eos_token_id,
            )
            answer = self.tokenizer.decode(output[0], skip_special_tokens=True)
            if "<|begin_of_text|>" in answer:
                answer = answer.split("<|begin_of_text|>")[-1]
            if "Assistant:" in answer:
                answer = answer.split("Assistant:")[-1].strip()
            return answer
        except Exception as e:
            return f"Error generating answer: {e}"
