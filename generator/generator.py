import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from logger.logger import get_logger, log_model_loading_metrics, log_generation_metrics
from utils.metrics.generator_metrics import GeneratorMetrics

logger = get_logger("generator")

class GeneratorStub:
    """
    Stub generator that returns a static answer.
    """
    def __init__(self):
        self.metrics = GeneratorMetrics()
        logger.info("GeneratorStub initialized")
    
    def generate(self, context, question):
        logger.info("Generating answer with GeneratorStub")
        
        # Start tracking
        context_length = sum(len(c) for c in context) if context else 0
        question_length = len(question)
        gen_id = self.metrics.start_generation(context_length, question_length)
        
        start_time = time.time()
        answer = "Answer generated by the LLM (stub)"
        duration = time.time() - start_time
        
        # Finish tracking
        self.metrics.finish_generation(gen_id, len(answer), duration)
        
        return answer, duration

class LLMGenerator:
    """
    Generator that uses a Hugging Face LLM to generate answers.
    """
    def __init__(self, model_id, max_tokens=250, temperature=0.7, max_gpu_memory="3.8GB"):
        logger.info(f"Loading LLM model: {model_id}")
        self.metrics = GeneratorMetrics()
        
        try:
            start_load = time.time()
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id, torch_dtype=torch.bfloat16, device_map="auto", max_memory={0: max_gpu_memory}
            )
            
            load_duration = time.time() - start_load
            log_model_loading_metrics(logger, load_duration)
            
            # Log GPU information
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                logger.info(f"GPU: {gpu_name} {gpu_memory:.2f}GB")
            else:
                logger.info("GPU: Not available, using CPU")
                
        except Exception as e:
            logger.error(f"Error loading LLM model: {e}")
            raise RuntimeError(f"Failed to load LLM model '{model_id}': {e}")
        
        self.max_tokens = max_tokens
        self.temperature = temperature

    def generate(self, context, question):
        logger.info("Generating answer with LLM")
        gen_id = None  # Initialize gen_id
        
        try:
            # Start tracking
            context_length = sum(len(c) for c in context) if context else 0
            question_length = len(question)
            gen_id = self.metrics.start_generation(context_length, question_length)
            
            if not context or len(context) == 0:
                # LLM Mode - No context provided
                prompt = f"Question: {question}\nAnswer:"
            else:
                # RAG Mode - Using only provided context
                context_text = "\n\n".join(context)
                prompt = f"""Based ONLY on the following context, answer the question. If the answer cannot be found in the context, say "I cannot answer this question based on the provided context."
                \nContext:\n{context_text}\nQuestion: {question}\nAnswer:"""
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            # Count input tokens
            input_tokens = inputs.input_ids.shape[1]
            logger.info(f"Input tokens: {input_tokens}")
            
            start_time = time.time()
            output = self.model.generate(
                **inputs,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            # Calculate metrics
            generation_time = time.time() - start_time
            output_tokens = output.shape[1] - input_tokens  # Only new tokens
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            answer = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # Remove prompt from answer if present
            if answer.startswith(prompt):
                answer = answer[len(prompt):].strip()
            
            # Log generation metrics (existing)
            log_generation_metrics(logger, generation_time)
            logger.info(f"Generated tokens: {output_tokens}, Generation time: {generation_time:.2f}s, Speed: {tokens_per_second:.2f} tokens/s")
            print(f"Generated tokens: {output_tokens}\nGeneration time: {generation_time:.2f}s\nSpeed: {tokens_per_second:.2f} tokens/s")
            
            # Finish tracking with token metrics
            self.metrics.finish_generation(gen_id, len(answer), generation_time, output_tokens)

            return answer, generation_time
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            
            # Log error in metrics
            if gen_id is not None:
                self.metrics.log_error(gen_id, str(e))
            
            return f"Error generating answer: {e}", 0