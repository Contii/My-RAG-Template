from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time
from logger.logger import get_logger

logger = get_logger("generator")

class GeneratorStub:
    """
    Stub generator that returns a static answer.
    """
    def generate(self, context, question):
        logger.info("Generating answer with GeneratorStub")
        return "Answer generated by the LLM (stub)"

class LLMGenerator:
    """
    Generator that uses a Hugging Face LLM to generate answers.
    """
    def __init__(self, model_id, max_tokens=250, temperature=0.7, max_gpu_memory="3.8GB"):
        logger.info(f"Loading LLM model: {model_id}")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id, torch_dtype=torch.bfloat16, device_map="auto", max_memory={0: max_gpu_memory}
            )
            
            # Log GPU information
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                logger.info(f"GPU: {gpu_name} {gpu_memory:.2f}GB")
            else:
                logger.info("GPU: Not available, using CPU")
                
        except Exception as e:
            logger.error(f"Error loading LLM model: {e}")
            raise RuntimeError(f"Failed to load LLM model '{model_id}': {e}")
        self.max_tokens = max_tokens
        self.temperature = temperature

    def generate(self, context, question):
        logger.info("Generating answer with LLM")
        try:
            if not context or len(context) == 0:
                # LLM Mode - No context provided
                prompt = f"Question: {question}\nAnswer:"
            else:
                # RAG Mode - Using only provided context
                context_text = "\n\n".join(context)
                prompt = f"""Based ONLY on the following context, answer the question. If the answer cannot be found in the context, say "I cannot answer this question based on the provided context."
                \nContext:\n{context_text}\nQuestion: {question}\nAnswer:"""
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            # Count input tokens
            input_tokens = inputs.input_ids.shape[1]
            logger.info(f"Input tokens: {input_tokens}")
            
            start_time = time.time()
            output = self.model.generate(
                **inputs,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
            end_time = time.time()
            
            # Calculate metrics
            generation_time = end_time - start_time
            output_tokens = output.shape[1] - input_tokens  # Only new tokens
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            answer = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
            # Log generation metrics
            logger.info(f"Generated tokens: {output_tokens}, Generation time: {generation_time:.2f}s, Speed: {tokens_per_second:.2f} tokens/s")
            print(f"Generated tokens: {output_tokens}\nGeneration time: {generation_time:.2f}s\nSpeed: {tokens_per_second:.2f} tokens/s")

            return answer, generation_time
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            return f"Error generating answer: {e}", 0