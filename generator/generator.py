from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


class GeneratorStub:
    """
    Stub generator that returns a static answer.
    """

    def generate(self, context, question):
        return "Answer generated by the LLM (stub)"


class LLMGenerator:
    """
    Generator that uses a Hugging Face LLM to generate answers.
    """

    def __init__(self, model_id, max_tokens=250, temperature=0.7):
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_id, torch_dtype=torch.bfloat16, device_map="auto"
            )
        except Exception as e:
            raise RuntimeError(f"Failed to load LLM model '{model_id}': {e}")
        self.max_tokens = max_tokens
        self.temperature = temperature

    def generate(self, context, question):
        try:
            prompt = f"{' '.join(context)}\nQuestion: {question}\nAnswer:"
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            output = self.model.generate(
                **inputs, max_new_tokens=self.max_tokens, temperature=self.temperature
            )
            answer = self.tokenizer.decode(output[0], skip_special_tokens=True)
            return answer
        except Exception as e:
            return f"Error generating answer: {e}"
