retriever_type: "smart"
data_path: "data/documents/"

# Generator Configuration
generator:
  type: "huggingface"  # Options: stub, huggingface (future: ollama, gguf, vllm)

  # HuggingFace settings
  model_id: "microsoft/bitnet-b1.58-2B-4T"
  max_tokens: 250
  temperature: 0.7
  max_gpu_memory: "3.8GB"  # Approx 95% of 4GB GPU

  # Future - Ollama example:
  # type: "ollama"
  # model: "llama3"
  # base_url: "http://localhost:11434"
  # temperature: 0.7
  # max_tokens: 250


# Smart Retriever configuration
retrieval:
  model_name: "all-MiniLM-L6-v2"
  top_k: 3
  embeddings_path: "data/embeddings"
  
  # Enable/disable components
  use_reranking: true
  use_cache: true
  use_filters: true
  use_metrics: true
  
  # Component configurations
  reranker_model: "cross-encoder/ms-marco-MiniLM-L12-v2"
  rerank_top_k: 10
  cache_ttl_hours: 24
  min_score_threshold: 0.3

  faiss:
    dimension: 384 # Ensure this matches your embedding model output dimension
    metric: "inner_product"  # Options: "inner_product", "l2"


# Logging configuration
logging:
  level: INFO
  log_to_file: true
  log_file: "logs/rag.log"
  format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"